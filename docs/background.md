# Biological & Theoretical Background

## A Biological Primer

A core concept of molecular biology is that a biomolecule's function is largely governed by its sequence of residues. The amino acids of a protein, or nucleotides of a ribonucleic acid (RNA), determine the molecule's structure, reactivity and specificity. As such, a major challenge that remains in biomedical research is understanding how mutations affect biomolecule function to the point that one can model and predict the effect of any particular mutation to any given protein or RNA.

For fundamental biological research, this would help elucidate the biochemical rules and constraints that govern biomolecular function. For medicine, this could provide vital information for understanding which genetic variants may be associated with disease. For biotechnology, this could provide vital aid to both the development of modified proteins with beneficial proteins and the construction of large molecular libraries enriched with functional sequences.

There is a clear need to be able to rapidly assess whether a given mutation will affect a biomolecule's function. However, assessing this experimentally is difficult and expensive. Technological advances have facilitated what is currently the best experimental option - high-throughput experimental assays known as [deep mutational scans](https://www.nature.com/articles/nmeth.3027), which can assess the effect of thousands of mutations on molecular, cellular or organismal fitness under selection pressure. However, this approach is very resource-intensive and still only manages to explore a fraction of a biomolecule's exponentially large sequence space, meaning that systematic exploration of all mutation combinations is not possible.

Instead, numerous computational models have been developed to tackle this problem, mostly by leveraging the fact that evolution has been applying a process of massively parallel mutagenesis and selection that can be thought of as a deep mutational scan that has been going on for millions of years. By analysing the natural sequence variation of biomolecule families, [one can infer functional and structural information](https://www.nature.com/articles/nbt.2419) for the biomolecule. 

This can also be applied to mutation effect prediction. The majority of previous models overcome the vast combinatorial complexity of sequence interactions by considering residues independently or only in the context of pairwise interactions. In the years preceding this project, the Marks lab took a different approach; replacing the explicit modelling of interactions with unique free parameters with implicitly capturing these interactions with latent variables; more specifically, capturing the problem's complexity with approximate inference using a bayesian deep latent-variable model, which is discussed below.

## Statistical Model Overview

### Summary

The model constructed, in its full form, is a Bayesian Variational Autoencoder (B-VAE) that models both the latent dimension (the encoder's output) and the decoder's parameters as variational parameters sampled from a gaussian distribution. Augmenting this variational approximation architecture, the final layer of the decoder is subject to a width-one convolution, a structured sparsity prior, and a final global temperature parameter. The non-Bayesian, standard VAE version of the model is also subject to dropout and $l2$ regularisation.

The model works by learning a latent variable representation of a biomolecule family's sequence alignments.

### Detail

Probabilistic latent-variable models such as variational autoencoders reveal structure hidden in data by first positing that the data is drawn from a hidden generative process and then using inference to learn the parameters of this process's distribution. Here, the data, $x$, is the set of sequences alignments for a particular biomolecule family. The latent variable model posits that this data is generated by some evolutionary process, $z$, (of dimension $D$) and the process of generating $x$ from $z$ is parameterized by $\theta$, such that:

$$ z \sim \mathcal{N}(0,I_D) $$

$$ x \sim p(x|z,\theta) $$

In our case, we assume this latent distribution $z$ from which $x$ is generated is a multivariate standard normal, and what we are interested in finding is $theta$, the parameters of the hidden generative process that maps $z$ to $x$ - we seek to find $\theta$ that fits p(x|\theta) to the observed data. In finding this, we can consider the following ratio:

$\log\frac{p(x^{\text{Mutant}}|\theta}{p(x^{\text{Wild-type}}|\theta} 

As a heuristic for determining the relative favorability of the Mutant sequence x^{\text{Mutant} against the wild-type sequence x^{\text{Wild-type}. This can be expressed as: given the evolutionary parameters that generate the observed sequences, $\theta$, what is the likelihood of observing this mutated sequence compared to observing the wild type sequence. The assumption behind this model's power is that a mutational sequence that is unlikely to be generated from the evolutionary process that generates the sequence family in question is also unlikely to be as functional or fit as the wild-type sequence. In a sense, the generative process defined by $\theta$ governs the structure and constraints of what defines the data as a functioning sequence family - mutated sequences that violate this process are unlikely to be viable sequences.

Since $z$ is hidden (we do not know a priori which $z$ variables are responsible for which $x$ variables), we must model $p(x|\theta)$ through a marginal likelihood:

$$ p(x|\theta) = \int p(x|z,\theta)p(z)dz

Direct computation of this is intractable. The trick common to all variational autoencoders is to instead deal with this integral's lower bound, the Evidence Lower Bound (ELBO), $\mathcal{L}(\phi)$ given as:

$$\mathcal{L}(\phi)=\mathbb{E}_q[\logp(x|z,\theta)]-D_KL[q(z|x,\phi)||p(z)]$$

Where $q(z|x,\phi)$ is the variational approximation of hidden variables given the observed variables. This technique is integral to all VAEs. Maximising the ELBO can intuitively be thought of as minimizing both the reconstruction loss of the autoencoder, whilst also minimising the Kullback-Leibler distance between the variational approximation of the distribution of $z$ given $x$ with the prior distribution of $z$.

Using neural networks (and the reparameterisation trick), we can model both $q(z|x,\phi)$ and $p(x|z,\theta)$ - the model's encoder takes in x and learns $q(z|x,\phi)$, the model's decoder samples from $q(z|x,\phi)$ and learns $p(x|z,\theta)$ through the following process:

$p(x_i = a|z) = \frac{e^{f(z_a^i)}}{\sum_be^{f(z_b^i)}}$ for $i = 1, ..., L$

Where $x_i$ is the $i$th element of sequence $x$ of length $L$, and $f(z)$ represents the non-linear neural network function approximation of the function parameterized by $\theta$. It is important to note that each element of the sequence, $x_i$, is conditionally independent of all other element given $z$; in other words, correlations between elements are mediated only by the latent variables.


















 Probabilistic latent-variable models reveal
structure in data by positing a partially unobserved generative process that created
the data and then conducting inference to learn the parameters of the generative
process. We focus on models in which an unobserved set of factors z are drawn
from an independent distribution and each data point arises according to a
conditional distribution p(x|z,θ) that is parameterized by θ. This process can be
written as
z I ∼ N(0, ) D
x x ∼ | p( , z θ)
Principal component analysis (PCA) has been a foundational model for the
analysis of genetic variation and can be realized in this probabilistic framework as
the zero-noise limit of probabilistic PCA43,84. With linear conditional dependencies
p(x|z,θ), PCA can model only additive interactions between the latent factors z.
This limitation could in principle be remedied through the use of a conditional
model p(x|z,θ) with nonlinear dependencies on z.
Here we consider a conditional model for sequences p(x|z,θ) that differs
from PCA in two ways. First, the conditional distribution of the data p(x|z,θ) 




## Contents

- [Abstract](index.md)
- [Project Motivation](motivation.md)
- [Biological & Theoretical Background](background.md)
- [Model Structure](structure.md)
- [Usage](usage.md)
- First Steps: Pyro
- Model Reconstruction
- Performance Comparison
- [Conclusions](conclusions.md)